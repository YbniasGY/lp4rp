\documentclass[a4paper]{article}
\usepackage[hyperref]{xcolor}
\definecolor{darkblue}{rgb}{0, 0, .4}
\usepackage[colorlinks=true,linkcolor=darkblue]{hyperref}
\usepackage{amsmath}
\usepackage[round]{natbib}
\usepackage{noweb}
\noweboptions{english,longxref}

\begin{document}

\title{R Package Development by Means of \\
  Literate Programming \texttt{(noweb)}}
\author{Bernhard Pfaff}
\date{\today}

\maketitle

\section{Introduction}


\section{Detecting Peaks/Troughs}

\subsection{Notation}
A uniformly sampled time series $\mathbf{y} = \{y_1, \ldots, y_i,
\ldots, y_T\}$ with $T$ data points is considered. The detection of
peak/trough points is achieved by a function $S(i, y_i, T)$ that
returns for data point $y_i$ a score value.\footnote{It suffices to
  provide a score function for peaks only. Trough points can be
  detected by using the negative values of the series $\mathbf{y}$.}.
If this score value surpasses a user-provided threshhold value
$\theta$, /i.e/, $S(i, y_i, T) \ge \theta$ then the point is
considered as a local peak/trough.\par

Furthermore, in case local peak/trough points appear closely together
with respect to time (clustered), then these points can be classified
as a burst or bust, respectively.

\subsection{Algorithms}
In \citet{PAL2009} five different score functions $S$ have been
suggested. All have in commom, that a centred window of size $2*k +
1 $ around $y_i$ is considered. That is, for a positive integer $k$
the $k$ right neigbours $N^+(i,k, T) = \{y_{i+1}, \ldots, y_{i + k}\}$
and the $k$ left neighbours $N^-(i, k, T) = \{\{y_{i-k}, \ldots,
y_{i-1}\}$ are employed for assessing $y\_i$ as a local
peak/trough. The union of $N^-(i, k, T)$ and $N^+(i, k, T)$ is defined
as $N(i, k, T) = N^-(i, k, T) \mathord{\cdot} N^+(i, k, T)$ and if the
centre point is included as $N'(i, k, T) = N^-(i, k, T) \mathord{\cdot}
y_i \mathord{\cdot} N^+(i, k, T)$.\par

The first function, $S_1$, computes the score value as the average of
the maximum differences between $y_i$ with its left and right
neighbours. The function is defined as:

\begin{equation}
\label{S1}
S_1 =
  \frac{\max{(y_i - y_{i-1}, \ldots, y_i - y_{i-k})} + \max{(y_i -
  y_{i+1}, \ldots, y_i - y_{i+k})}}{2}
\end{equation}

The equation~\eqref{S1} can be casted in R as:

<<score-maxdiff>>=
scmaxdiff <- function(x, k){
    cp <- k + 1L
    lmax <- max(x[cp] - head(x, k))
    rmax <- max(x[cp] - tail(x, k))
    (lmax + rmax) / 2.0
}
@ %def scmaxdiff

Instead of using the maximum differences of $y_i$ with its $k$ left
and right neighbours as in~\eqref{S1}, an alternative is to compute
the mean differences and evaluate the average thereof:

\begin{equation}
\label{S2}
S_2 = \frac{\frac{(y_i - y_{i-1}, \ldots, y_i - y_{i-k})}{k} +
\frac{(y_i - y_{i+1}, \ldots, y_i - y_{i+k})}{k}}{2}
\end{equation}

This equation can be casted in R as:

<<score-diffmean>>=
scdiffmean <- function(x, k){
    cp <- k + 1L
    ldmean <- x[cp] - mean(head(x, k))
    rdmean <- x[cp] - mean(tail(x, k))
    (ldmean + rdmean) / 2.0
}
@ %def scdiffmean

Another variation of score computation that has been proposed by
\citet{PAL2009} is to consider the differences to the mean of the $k$
left and right neighbours, that is:

\begin{equation}
\label{S3}
S_3 = \frac{
(y_i - \frac{(y_{i-1}, \ldots, y_{i-k})}{k}) +
(y_i - \frac{(y_{i+1}, \ldots, y_{i+k})}{k})}{2}
\end{equation}

The equation~\eqref{S3} can be casted as R function [[scavgdiff]]
for instance as follows:

<<score-avgdiff>>=
scavgdiff <- function(x, k){
    cp <- k + 1L
    lmean <- mean(x[cp] - head(x, k))
    rmean <- mean(x[cp] - tail(x, k))
    (lmean + rmean) / 2.0
}
@ %def scavgdiff

The fourth proposed score function differs from the previous three in
the sense that it does take explicitly the differences between $y_i$
and its neighbours explicitly into account, but tries to capture its
information content by means of relative entropy. The entropy of a
vector $A$ with elements $A = \{a_1, \ldots, a_m\}$ is given as:

\begin{equation}
H_w(A) = \sum_{i=1}^M \left(-p_w(a_i)\log(p_w(a_i))\right)
\end{equation}

where $p_w(a_i)$ is an estimate of the density value at $a_i$. The
score function is now based on computing the entropies of $H(N((k, i,
T))$ and $H(N'(k, i, T))$. Hereby, the densities can be determined by
means of a kernel density estimator. The score function is then
defined as the difference of the entropies:

\begin{equation}
\label{S4}
S_4 = H(N((k, i, T)) - H(N'((k, i, T))
\end{equation}

This concept is implemented in the function [[scentropy()]]. The
empirical density is computed by calling [[density()]]. The ellipsis
argument of [[scentropy()]] is passed down to this function and hereby
allowing the user to employ other than the default arguments of
[[density()]].

<<score-entropy>>=
scentropy <- function(x, k, ...){
    cp <- k + 1L
    dfull <- density(x, ...)$y
    hfull <- sum(-dfull * log(dfull))
    dexct <- density(x[-cp], ...)$y
    hexct <- sum(-dexct * log(dexct))
    hfull - hexct
}
@ %def scentropy

Finally, a moment-based score function has been put forward in the
article by Palshikar. Hereby, the first and second moment of $N((k, i,
T))$ are computed and a t-type statistic can be computed as $(y_i - m)
/ s$. If this statistic surpasses a provided threshhold $h$, then the
data point is considered as a local peak/trough.

\begin{equation}
\label{S5}
S_5 =
\begin{cases}
1 & (y_i - m) / s \ge h \\
0 & \text{else}
\end{cases}
\end{equation}

This type of scoring algorithm is implemented as function
[[scttype()]] below:

<<score-ttype>>=
scttype <- function(x, k, tval){
    cp <- k + 1L
    m <- mean(x[-cp])
    s <- sd(x[-cp])
    tstat <- (x[cp] - m) / s
    if ( abs(tstat) < tval ){
        tstat <- 0
    }
    tstat
}
@ %def scttype

Incidentally, an ensemble forecast of these five algorithms can be
utilized for local peak/trough classification can be employed. Hereby,
one could either use a hybrid approach, whereby only those
data points are considered as peak/trough points, if all five methods
coincide. This concept is casted in the function
[[schybrid()]]. Hereby, the signs of all five scoring algorithm are
tested for equality.

<<score-hybrid>>=
schybrid <- function(x, k, tval, ...){
    s <- c(sign(scmaxdiff(x, k)),
           sign(scavgdiff(x, k)),
           sign(scdiffmean(x, k)),
           sign(scentropy(x, k, ...)),
           sign(scttype(x, k, tval)))
    val <- unique(s)
    if ( length(val) < 2 ){
        return(s[1])
    } else {
        return(0)
    }
}
@ %def schybrid

It is also conceivable to base the classification on a majority
vote. For instance, if three out of the five algorithm classify a data
point as a local peak/trough, then this is taken as sufficient
evidence. This approach is defined in the function [[scvote()]] below.
The count of same 'votes' is set by the argument \texttt{confby}. Its
default value is $3$, \emph{i.e.} a simple majority. For
\texttt{confby = 5} the function would return the same classification
as [[schybrid()]] does.

<<score-vote>>=
scvote <- function(x, k, tval, confby = 3, ...){
    s <- c(sign(scmaxdiff(x, k)),
           sign(scavgdiff(x, k)),
           sign(scdiffmean(x, k)),
           sign(scentropy(x, k, ...)),
           sign(scttype(x, k, tval)))
    pos <- rep(1, 5)
    zer <- rep(0, 5)
    neg <- rep(-1, 5)
    spos <- sum(s == pos)
    szer <- sum(s == zer)
    sneg <- sum(s == neg)
    v <- c(spos, szer, sneg)
    idx <- which(v >= confby)
    vals <- c(1, 0, -1)
    if ( length(idx) > 0 ){
        return(vals[idx])
    } else {
        return(0)
    }
}
@ %def scvote

\subsection{Combining score methods}

<<score-wrapper>>=
score <- function(x, k,
                  scoreby = c("vote", "avg", "diff", "max", "ent",
                              "ttype", "hybrid"),
                  tval = 1.0, confby = 3, ...){
    scoreby <- match.arg(scoreby)
    ans <- switch(scoreby,
                  vote = scvote(x, k, tval, confby, ...),
                  avg = scavgdiff(x, k),
                  diff = scdiffmean(x, k),
                  max = scmaxdiff(x, k),
                  ent = scentropy(x, k, ...),
                  ttype = scttype(x, k, tval),
                  hybrid = schybrid(x, k, tval, ...)
                  )
    ans
}
@ %def score

The content/structure of the file \texttt{score.R} is given as:

<<score.R>>=
<<man-func-score>>
<<score-wrapper>>
#' @rdname score
<<score-maxdiff>>
#' @rdname score
<<score-diffmean>>
#' @rdname score
<<score-avgdiff>>
#' @rdname score
<<score-entropy>>
#' @rdname score
<<score-ttype>>
#' @rdname score
<<score-hybrid>>
#' @rdname score
<<score-vote>>
@

Within this file, all score-related methods and the wrapper-function
\texttt{score()} is included. The function definitions are
interspersed with the roxygen tags, which will be parsed to the
\texttt{Rd}-file \texttt{score.Rd}.


\section{Package structure}


\subsection{Preliminaries}

First, a skeleton of the package

<<DESCRIPTION.R>>=
Package: hiker
Title: Local Peak and Trough of a Time Series
Version: 0.0.0.9000
Authors@R: person("Bernhard", "Pfaff", email = "bernhard@pfaffikus.de",
                  role = c("aut", "cre"))
Description: Methods for detecting local peaks and troughs of a time series.
Depends: R (>= 3.3.1), zoo, methods
License: GPL-3
Encoding: UTF-8
LazyData: true
@



\subsection{Import directives and S4-classes}

<<Allclasses.R>>=
#' @import methods
NULL
#' @import zoo
NULL
#' @importFrom stats density sd na.omit start end smooth
NULL
#' @importFrom utils head tail
NULL

# Setting old (aka S3) classes
setOldClass("zoo")


<<man-class-HikeR>>
setClass("HikeR", slots = list(ys = "zoo",
                               k = "integer",
                               scoreby = "character",
                               yname = "character"))

<<man-class-PTBB>>
setClass("PTBB", slots = list(pt = "zoo",
                              type = "character",
                              h = "numeric"))
@

\section{Appendix}

\subsection{Roxygen Documentation}

\subsection{Documentation of functions}

<<man-func-score>>=
#' Basic scoring methods for local minima and maxima
#'
#' These are basic functions for evaluating the centre
#' point of a time series as local minimum or maximum.
#' Hereby, a score value is computed according to various methods.
#' If the score is positive, the centre point is tentatively
#' classified as a local peak.
#' Incidentally, negative scores indicate a local minima.
#'
#' @param x \code{numeric}, vector of length \code{2 * k + 1}.
#' @param k \code{integer}, the count of left/right neighbours.
#' @param scoreby \code{character}, the scoring method to be used.
#' @param tval \code{numeric}, factor for standard deviation band
#' if \code{scoreby = 'ttype'}.
#' @param confby \code{integer}, count of minimum vote,
#' values in the set \code{3:5}.
#' @param ... ellipsis argument.
#'
#' @name score
#' @family scores
#' @return \code{numeric}, the score value.
NULL

#' @rdname score
#' @export
@

\subsection{Documentation of S4-classes}

<<man-class-HikeR>>=
#' S4 class \code{HikeR}
#'
#' Formal class for classifying local minima and maxima
#' of a time series.
#'
#' @slot ys \code{zoo}, time series with associated scores.
#' @slot k \code{integer}, count of left/right neigbours around centre point.
#' @slot scoreby \code{character}, scoring method.
#' @slot yname \code{character}, name of the series.
#' @exportClass HikeR
@

<<man-class-PTBB>>=
#' S4 class \code{PTBB}
#'
#' Formal class for peaks, troughs, burst, busts and
#' intermittent phase of a time series.
#'
#' @slot pt \code{zoo}, logical: indicating peak/trough points.
#' @slot type \code{character}, type of point/phase.
#' @slot h \code{numeric}, the threshhold for score evaluation.
#' @exportClass PTBB
@

\subsection{Makefile}


\newpage
\section{Chunk Index}
\nowebchunks

\newpage
\section{Identifier Index}
\nowebindex

\newpage
\bibliographystyle{chicago}
\bibliography{hiker}

\end{document}
